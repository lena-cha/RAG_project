import streamlit as st

from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import ChatMessage

from dotenv import load_dotenv
load_dotenv()

# handle streaming conversation
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

# Function to extract text from an PDF file
from pdfminer.high_level import extract_text

def get_pdf_text(filename):
    raw_text = extract_text(filename)
    return raw_text

# document preprocess
def process_uploaded_file(uploaded_file):
    # Load document if file is uploaded
    if uploaded_file is not None:
        # loader - PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        raw_text = get_pdf_text(uploaded_file)

        # splitter - í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        # splitter
        text_splitter = CharacterTextSplitter(
        separator = "\n\n",
       chunk_size = 1000,
       chunk_overlap  = 200,
       length_function = len
        )
        all_splits = text_splitter.create_documents([raw_text])

        print("ì´ " + str(len(all_splits)) + "ê°œì˜ passage")
        
        # storage - ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        vectorstore = FAISS.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())
               
        return vectorstore, raw_text
    return None

# generate response using RAG technic
def generate_response(query_text, vectorstore, callback):

    # retriever - ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    docs_list = vectorstore.similarity_search(query_text, k=3)
    docs = ""
    for i, doc in enumerate(docs_list):
        docs += f"'ë¬¸ì„œ{i+1}':{doc.page_content}\n"
        
    # generator - ChatOpenAI ëª¨ë¸ ìƒì„±
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, streaming=True, callbacks=[callback])
    
    # chaining - í”„ë¡¬í”„íŠ¸ì™€ í•¨ê»˜ LLM í˜¸ì¶œ
    rag_prompt = [
        SystemMessage(
            content="ë„ˆëŠ” ë¬¸ì„œì— ëŒ€í•´ ì§ˆì˜ì‘ë‹µì„ í•˜ëŠ” 'ë¬¸ì„œë´‡'ì´ì•¼. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€ì„ í•´ì¤˜. ë¬¸ì„œì— ë‚´ìš©ì´ ì •í™•í•˜ê²Œ ë‚˜ì™€ìˆì§€ ì•Šìœ¼ë©´ ëŒ€ë‹µí•˜ì§€ ë§ˆ."
        ),
        HumanMessage(
            content=f"ì§ˆë¬¸:{query_text}\n\n{docs}"
        ),
    ]

    response = llm(rag_prompt)

    return response.content
    


def generate_summarize(raw_text, callback):
    # ChatOpenAI ëª¨ë¸ ìƒì„±
    llm = ChatOpenAI(
        model_name="gpt-3.5-turbo",
        temperature=0,
        streaming=True,
        callbacks=[callback]
    )
    
    # ìš”ì•½ í”„ë¡¬í”„íŠ¸
    system_message = SystemMessage(content="""ë‹¹ì‹ ì€ ë¬¸ì„œ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
    ì£¼ì–´ì§„ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
    ì¤‘ìš”í•œ í¬ì¸íŠ¸ë“¤ì„ ë¹ ëœ¨ë¦¬ì§€ ì•Šìœ¼ë©´ì„œë„ í•µì‹¬ë§Œ ì¶”ë ¤ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.""")
    
    human_message = HumanMessage(content=f"""
    ë‹¤ìŒ ë¬¸ì„œë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:
    
    {raw_text[:4000]}  # í† í° ì œí•œì„ ê³ ë ¤í•˜ì—¬ ì¼ë¶€ë§Œ ì‚¬ìš©
    
    ìœ„ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
    """)
    
    response = llm([system_message, human_message])
    
    return response.content


# page title
st.set_page_config(page_title='ğŸ¦œğŸ”— ë¬¸ì„œ ê¸°ë°˜ ìš”ì•½ ë° QA ì±—ë´‡')
st.title('ğŸ¦œğŸ”— ë¬¸ì„œ ê¸°ë°˜ ìš”ì•½ ë° QA ì±—ë´‡')

# file upload
uploaded_file = st.file_uploader('Upload an document', type=['pdf'])

# file upload logic
if uploaded_file:
    vectorstore, raw_text = process_uploaded_file(uploaded_file)
    if vectorstore:
        st.session_state['vectorstore'] = vectorstore
        st.session_state['raw_text'] = raw_text
        
# chatbot greatings
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(
            role="assistant", content="ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ë¬¸ì„œì— ëŒ€í•œ ì´í•´ë¥¼ ë„ì™€ì£¼ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. ì–´ë–¤ê²Œ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"
        )
    ]

# conversation history print 
for msg in st.session_state.messages:
    st.chat_message(msg.role).write(msg.content)
    
# message interaction
if prompt := st.chat_input("'ìš”ì•½'ì´ë¼ê³  ì…ë ¥í•´ë³´ì„¸ìš”!"):
    st.session_state.messages.append(ChatMessage(role="user", content=prompt))
    st.chat_message("user").write(prompt)

    with st.chat_message("assistant"):
        stream_handler = StreamHandler(st.empty())
        
        if prompt == "ìš”ì•½":
            response = generate_summarize(st.session_state['raw_text'],stream_handler)
            st.session_state["messages"].append(
                ChatMessage(role="assistant", content=response)
            )
        
        else:
            response = generate_response(prompt, st.session_state['vectorstore'], stream_handler)
            st.session_state["messages"].append(
                ChatMessage(role="assistant", content=response)
            )